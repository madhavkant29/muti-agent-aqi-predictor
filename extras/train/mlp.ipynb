{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "\n",
        "# --- Paths ---\n",
        "DATASET_PATH = \"/content/sample_data/main.csv\"\n",
        "MODEL_SAVE_PATH = \"mlp_pollutants_model.pth\"\n",
        "\n",
        "# --- Load Dataset ---\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at: {DATASET_PATH}\")\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "# --- Features ---\n",
        "weather_features = [\n",
        "    \"temperature_2m (°C)\",\n",
        "    \"relative_humidity_2m (%)\",\n",
        "    \"rain (mm)\",\n",
        "    \"wind_speed_100m (km/h)\",\n",
        "    \"wind_direction_100m (°)\",\n",
        "    \"pressure_msl (hPa)\",\n",
        "    \"surface_pressure (hPa)\"\n",
        "]\n",
        "\n",
        "pollutant_features = [\"co\", \"no\", \"no2\", \"o3\", \"so2\", \"pm2_5\", \"pm10\", \"nh3\"]\n",
        "\n",
        "X = df[weather_features].values\n",
        "y = df[pollutant_features].values\n",
        "\n",
        "# --- Scaling ---\n",
        "x_scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "X_scaled = x_scaler.fit_transform(X)\n",
        "y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "# --- Dataset class ---\n",
        "class WeatherPollutantDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --- Train/Test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = WeatherPollutantDataset(X_train, y_train)\n",
        "test_dataset = WeatherPollutantDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# --- Improved Model ---\n",
        "class ImprovedMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ImprovedMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.relu(self.bn1(self.fc1(x)))\n",
        "        x1 = self.dropout(x1)\n",
        "        x2 = self.relu(self.bn2(self.fc2(x1))) + x1  # residual connection\n",
        "        x2 = self.dropout(x2)\n",
        "        x3 = self.relu(self.bn3(self.fc3(x2))) + x2  # residual connection\n",
        "        x3 = self.dropout(x3)\n",
        "        out = self.fc_out(x3)\n",
        "        return out\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedMLP(input_dim=len(weather_features), hidden_dim=128, output_dim=len(pollutant_features)).to(device)\n",
        "\n",
        "# --- Automatic target weighting based on variance ---\n",
        "target_variance = torch.tensor(np.var(y_scaled, axis=0), dtype=torch.float32, device=device)\n",
        "weights = 1.0 / (target_variance + 1e-6)  # smaller variance gets higher weight\n",
        "weights = weights / weights.sum() * len(pollutant_features)  # normalize\n",
        "\n",
        "def weighted_mse(pred, target, weights):\n",
        "    return torch.mean(weights * (pred - target) ** 2)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "# --- Training ---\n",
        "epochs = 100\n",
        "patience = 15\n",
        "best_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = weighted_mse(outputs, y_batch, weights)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    scheduler.step(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.5f}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"Training complete. Best model saved.\")\n",
        "\n",
        "# --- Inference ---\n",
        "def predict_pollutants(model, weather_input):\n",
        "    model.eval()\n",
        "    weather_tensor = torch.tensor(weather_input, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        preds = model(weather_tensor).cpu().numpy()\n",
        "    return y_scaler.inverse_transform(preds)\n",
        "\n",
        "# --- Evaluation ---\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        preds = model(X_batch).cpu().numpy()\n",
        "        targets = y_batch.cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_targets.append(targets)\n",
        "\n",
        "all_preds = np.vstack(all_preds)\n",
        "all_targets = np.vstack(all_targets)\n",
        "all_preds_inv = y_scaler.inverse_transform(all_preds)\n",
        "all_targets_inv = y_scaler.inverse_transform(all_targets)\n",
        "\n",
        "mse = mean_squared_error(all_targets_inv, all_preds_inv)\n",
        "mae = mean_absolute_error(all_targets_inv, all_preds_inv)\n",
        "r2 = r2_score(all_targets_inv, all_preds_inv, multioutput=\"raw_values\")\n",
        "\n",
        "print(\"\\n--- Test Metrics ---\")\n",
        "print(f\"MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
        "for i, p in enumerate(pollutant_features):\n",
        "    print(f\"{p} R²: {r2[i]:.3f}\")\n",
        "\n",
        "# --- Example Predictions ---\n",
        "example_weather = X_test[:5]\n",
        "predicted_pollutants = predict_pollutants(model, example_weather)\n",
        "for i, p in enumerate(predicted_pollutants):\n",
        "    print(f\"Sample {i+1} Predicted pollutants: {p}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5tiOXkYnrfr",
        "outputId": "ee48db2f-c38e-453f-f4f7-a7a12e1c8684"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/100, Loss: 0.16491\n",
            "Epoch 2/100, Loss: 0.02336\n",
            "Epoch 3/100, Loss: 0.01140\n",
            "Epoch 4/100, Loss: 0.00968\n",
            "Epoch 5/100, Loss: 0.00950\n",
            "Epoch 6/100, Loss: 0.00944\n",
            "Epoch 7/100, Loss: 0.00934\n",
            "Epoch 8/100, Loss: 0.00947\n",
            "Epoch 9/100, Loss: 0.00937\n",
            "Epoch 10/100, Loss: 0.00941\n",
            "Epoch 11/100, Loss: 0.00948\n",
            "Epoch 12/100, Loss: 0.00936\n",
            "Epoch 13/100, Loss: 0.00931\n",
            "Epoch 14/100, Loss: 0.00926\n",
            "Epoch 15/100, Loss: 0.00925\n",
            "Epoch 16/100, Loss: 0.00911\n",
            "Epoch 17/100, Loss: 0.00905\n",
            "Epoch 18/100, Loss: 0.00906\n",
            "Epoch 19/100, Loss: 0.00904\n",
            "Epoch 20/100, Loss: 0.00903\n",
            "Epoch 21/100, Loss: 0.00897\n",
            "Epoch 22/100, Loss: 0.00885\n",
            "Epoch 23/100, Loss: 0.00887\n",
            "Epoch 24/100, Loss: 0.00884\n",
            "Epoch 25/100, Loss: 0.00883\n",
            "Epoch 26/100, Loss: 0.00867\n",
            "Epoch 27/100, Loss: 0.00863\n",
            "Epoch 28/100, Loss: 0.00859\n",
            "Epoch 29/100, Loss: 0.00841\n",
            "Epoch 30/100, Loss: 0.00838\n",
            "Epoch 31/100, Loss: 0.00837\n",
            "Epoch 32/100, Loss: 0.00837\n",
            "Epoch 33/100, Loss: 0.00834\n",
            "Epoch 34/100, Loss: 0.00826\n",
            "Epoch 35/100, Loss: 0.00827\n",
            "Epoch 36/100, Loss: 0.00832\n",
            "Epoch 37/100, Loss: 0.00827\n",
            "Epoch 38/100, Loss: 0.00821\n",
            "Epoch 39/100, Loss: 0.00822\n",
            "Epoch 40/100, Loss: 0.00813\n",
            "Epoch 41/100, Loss: 0.00818\n",
            "Epoch 42/100, Loss: 0.00819\n",
            "Epoch 43/100, Loss: 0.00802\n",
            "Epoch 44/100, Loss: 0.00813\n",
            "Epoch 45/100, Loss: 0.00800\n",
            "Epoch 46/100, Loss: 0.00806\n",
            "Epoch 47/100, Loss: 0.00806\n",
            "Epoch 48/100, Loss: 0.00809\n",
            "Epoch 49/100, Loss: 0.00798\n",
            "Epoch 50/100, Loss: 0.00806\n",
            "Epoch 51/100, Loss: 0.00804\n",
            "Epoch 52/100, Loss: 0.00797\n",
            "Epoch 53/100, Loss: 0.00792\n",
            "Epoch 54/100, Loss: 0.00801\n",
            "Epoch 55/100, Loss: 0.00801\n",
            "Epoch 56/100, Loss: 0.00798\n",
            "Epoch 57/100, Loss: 0.00795\n",
            "Epoch 58/100, Loss: 0.00803\n",
            "Epoch 59/100, Loss: 0.00802\n",
            "Epoch 60/100, Loss: 0.00783\n",
            "Epoch 61/100, Loss: 0.00779\n",
            "Epoch 62/100, Loss: 0.00774\n",
            "Epoch 63/100, Loss: 0.00777\n",
            "Epoch 64/100, Loss: 0.00777\n",
            "Epoch 65/100, Loss: 0.00774\n",
            "Epoch 66/100, Loss: 0.00780\n",
            "Epoch 67/100, Loss: 0.00781\n",
            "Epoch 68/100, Loss: 0.00778\n",
            "Epoch 69/100, Loss: 0.00771\n",
            "Epoch 70/100, Loss: 0.00766\n",
            "Epoch 71/100, Loss: 0.00765\n",
            "Epoch 72/100, Loss: 0.00763\n",
            "Epoch 73/100, Loss: 0.00764\n",
            "Epoch 74/100, Loss: 0.00766\n",
            "Epoch 75/100, Loss: 0.00767\n",
            "Epoch 76/100, Loss: 0.00759\n",
            "Epoch 77/100, Loss: 0.00767\n",
            "Epoch 78/100, Loss: 0.00760\n",
            "Epoch 79/100, Loss: 0.00767\n",
            "Epoch 80/100, Loss: 0.00760\n",
            "Epoch 81/100, Loss: 0.00758\n",
            "Epoch 82/100, Loss: 0.00763\n",
            "Epoch 83/100, Loss: 0.00766\n",
            "Epoch 84/100, Loss: 0.00763\n",
            "Epoch 85/100, Loss: 0.00761\n",
            "Epoch 86/100, Loss: 0.00759\n",
            "Epoch 87/100, Loss: 0.00758\n",
            "Epoch 88/100, Loss: 0.00761\n",
            "Epoch 89/100, Loss: 0.00757\n",
            "Epoch 90/100, Loss: 0.00755\n",
            "Epoch 91/100, Loss: 0.00757\n",
            "Epoch 92/100, Loss: 0.00757\n",
            "Epoch 93/100, Loss: 0.00757\n",
            "Epoch 94/100, Loss: 0.00762\n",
            "Epoch 95/100, Loss: 0.00762\n",
            "Epoch 96/100, Loss: 0.00764\n",
            "Epoch 97/100, Loss: 0.00752\n",
            "Epoch 98/100, Loss: 0.00750\n",
            "Epoch 99/100, Loss: 0.00756\n",
            "Epoch 100/100, Loss: 0.00753\n",
            "Training complete. Best model saved.\n",
            "\n",
            "--- Test Metrics ---\n",
            "MSE: 544697.000, MAE: 216.458\n",
            "co R²: 0.463\n",
            "no R²: 0.407\n",
            "no2 R²: 0.435\n",
            "o3 R²: 0.218\n",
            "so2 R²: 0.334\n",
            "pm2_5 R²: 0.530\n",
            "pm10 R²: 0.505\n",
            "nh3 R²: 0.395\n",
            "Sample 1 Predicted pollutants: [5728.5356     92.902725   96.481415   31.586817  104.51426   563.0409\n",
            "  652.47095    27.388975]\n",
            "Sample 2 Predicted pollutants: [3564.2605     43.908306   77.64689    77.04322    89.95708   255.73396\n",
            "  336.3228     29.1554  ]\n",
            "Sample 3 Predicted pollutants: [8318.626      142.3135     131.7268       9.9219885  102.39709\n",
            "  615.3316     764.89124     70.41417  ]\n",
            "Sample 4 Predicted pollutants: [3482.24       39.745712   85.523575  128.02       72.700005  270.2242\n",
            "  346.1188     34.36986 ]\n",
            "Sample 5 Predicted pollutants: [2280.5144     15.763297   44.85342    48.445793   36.517365  265.75317\n",
            "  298.2568      9.09259 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"\\n--- MSE and MAE per pollutant ---\")\n",
        "for i, pollutant in enumerate(pollutant_features):\n",
        "    mse_i = mean_squared_error(all_targets_inv[:, i], all_preds_inv[:, i])\n",
        "    mae_i = mean_absolute_error(all_targets_inv[:, i], all_preds_inv[:, i])\n",
        "    print(f\"{pollutant}: MSE = {mse_i:.3f}, MAE = {mae_i:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrmVjCHSnriM",
        "outputId": "8a47642d-2294-4aed-f152-b9626fda2b9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MSE and MAE per pollutant ---\n",
            "co: MSE = 4289033.500, MAE = 1363.769\n",
            "no: MSE = 2127.071, MAE = 27.503\n",
            "no2: MSE = 1321.070, MAE = 24.853\n",
            "o3: MSE = 5219.814, MAE = 50.384\n",
            "so2: MSE = 1576.203, MAE = 26.139\n",
            "pm2_5: MSE = 23385.590, MAE = 100.982\n",
            "pm10: MSE = 34485.324, MAE = 125.989\n",
            "nh3: MSE = 431.672, MAE = 12.047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Relative Accuracy (%) per pollutant (safe) ---\")\n",
        "for i, pollutant in enumerate(pollutant_features):\n",
        "    y_true = all_targets_inv[:, i]\n",
        "    y_pred = all_preds_inv[:, i]\n",
        "    # divide by mean of true values instead of individual values\n",
        "    acc = 100 * (1 - np.mean(np.abs(y_true - y_pred)) / (np.mean(y_true) + 1e-6))\n",
        "    print(f\"{pollutant}: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxBOWimenrk7",
        "outputId": "0b337a29-492d-48cf-f491-77ac1ca9e362"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relative Accuracy (%) per pollutant (safe) ---\n",
            "co: 52.78%\n",
            "no: 16.20%\n",
            "no2: 61.89%\n",
            "o3: 17.50%\n",
            "so2: 60.46%\n",
            "pm2_5: 57.01%\n",
            "pm10: 57.45%\n",
            "nh3: 51.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save scalers\n",
        "joblib.dump(x_scaler, \"x_scaler.pkl\")\n",
        "joblib.dump(y_scaler, \"y_scaler.pkl\")\n",
        "\n",
        "print(\"Feature and target scalers saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4HVML_Enrnj",
        "outputId": "897533da-30e1-4e8e-938b-806082dcc020"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature and target scalers saved successfully.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}